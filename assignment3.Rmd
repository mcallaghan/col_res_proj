---
title: 'Twitter and the European Hyperagora: What can the Twittersphere Tell us about Political Deliberation and Opinions in Europe?'
author: "M. Callaghan, V. Nieberg"
date: "November 14, 2015"
output: html_document
bibliography: 
- bibliography.bib 
- RpackageCitations3.bib
---

```{r include=FALSE}
pkgs <- c('twitteR', 'tm.lexicon.GeneralInquirer', 'tm.plugin.sentiment')
repmis::LoadandCite(pkgs, file = 'RpackageCitations3.bib')
```



# Data Gathering 

Due to the large amount of data we process, we ran the data gathering and cleaning in the background on a server using the prefix setsid.


## Tweets
We used a modified version of GetOldTweets [@GetOldTweets], a Java program that scrapes data from twitter search. The file getting_tweets/input.txt contains a list of search terms related to the Greek crisis in three periods, each comprising some weeks before and after the negotiation and signing of the memoranda. The search terms were collected using an adapted form of snowball sampling [@snowball], searching an initial list and recursively adding related terms found in the results. By running 
```
sudo setsid ./compile_run.sh ../getting_tweets/input.txt
```
from the GetOldTweets folder, we ran through each search term and each period, searched twitter, and saved the results as a txt file in the data folder. After an initial assessment of the results, we refined our search terms and ran GetOldTweets again with /getting_tweets/input2.txt. A third file (getting_tweets/input3.txt) aims to return a time-inpedependent list of tweets in order to control for the growth of Twitter over time.

We end up with a long list of files in the data/GOToutput folder, which in the data cleaning process will be merged into one corpus file.

## Users
We found the unique users in our corpus of tweets and used the TwitteR package [@R-twitteR] to gather richer data about each user. TwitteR uses the twitter API and gives the opportunity to collect all information twitter has abou the user. Where a users's last tweet was geocoded, we took the latitude and longtitude. We end up with the file data/user_info.csv

Many users do not geotag their tweets, instead stating their location, and we used APIs from MapQuest and Google to geocode user-reported location, giving us the file places.csv.


# Data Cleaning  
The txt files containing the tweets for each query and period are merged into a corpus file. This corpus file was merged with the user_info file, which in turn was merged with the places file. We end up with a large file containing tweets for our queries in each period with elaborete user information.

Some of the queries we defined returned irrelevant data, due to their ambiguity. We identified these by selecting random tweets from the search queries, reading the tweets, and checking for relevance to the topic. For example, the query "bailout", although certainly relevant for our topic, was insufficiently precise and returned a lot of data about the banking bailouts, especially in the 2010 period.

```{r}
user_info <- read.csv('data/user_info.csv')
corpus <- read.csv('data/corpus.csv')
geo_user_info <- user_info[!is.na(user_info$latitude), ]
geo_corpus <- merge(corpus, geo_user_info)
```


# Data Analysis

## Descriptive Statistics

## Summary Tables

## Timeless against other queries

## Summary Charts

```{r cache=TRUE}
library(ggplot2)
library(dplyr)
library(stringr)
library(wordcloud)
library(tm)
corpus <- read.csv('data/corpus.csv')
eurocrisis <- filter(corpus, query == 'eurocrisis')

corpus <- filter(corpus,query!="2-pac"&query!="3-pac")

eurocrisis$day <- as.Date(substr(as.character(eurocrisis$date),1,10))

eurocrisis_daily <- eurocrisis %>%
  group_by(day) %>%
  summarise(
    n = length(day)
  )

corpus$day <- as.Date(substr(as.character(corpus$date),1,10))

corpus_index <- corpus %>%
  group_by(query) %>%
  summarise(total = length(query))

corpus_index_large <- filter(corpus_index,total>10000)
corpus_index_small <- filter(corpus_index,total<10000)

corpus_large <- filter(corpus,query %in% corpus_index_large$query)

corpus_small <- filter(corpus,query %in% corpus_index_small$query)

timegraph <- function(corpus,p) {
  corpus_daily <- corpus %>%
    filter(period==p) %>%
  group_by(day,query,period) %>%
  summarise(
    n = length(day)
  )
  

ggplot(filter(corpus_daily), aes(day,n,colour=query)) + geom_line() +
  scale_x_date(format("%b-%Y")) +
  ggtitle(p)

}

timegraph(corpus_small,"2010-04-15-2010-05-15.txt")
timegraph(corpus_small,"2012-02-07-2012-03-31.txt")
timegraph(corpus_small,"2015-07-01-2015-08-25.txt")

timegraph(corpus_large,"2010-04-15-2010-05-15.txt")
timegraph(corpus_large,"2012-02-07-2012-03-31.txt")
timegraph(corpus_large,"2015-07-01-2015-08-25.txt")



```

## Word cloud
```{r cache = TRUE}
minicorpus <- corpus[sample(nrow(corpus), 200), ]

text_corpus <- Corpus(VectorSource(minicorpus$text))

#clean up
text_corpus <- tm_map(text_corpus,
                              content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')),
                              mc.cores=1
)
text_corpus <- tm_map(text_corpus, content_transformer(tolower), mc.cores=1)
text_corpus <- tm_map(text_corpus, removePunctuation, mc.cores=1)
text_corpus <- tm_map(text_corpus, function(x)removeWords(x,stopwords()), mc.cores=1)
wordcloud(text_corpus)

```

## Map
```{r}
library(rworldmap)
map_world <- getMap(resolution = "high")
plot(map_world, asp = 1)
points(geo_corpus$longtitude, geo_corpus$latitude, col = "red", cex = .6) 

map_europe <- getMap(resolution = "high")
plot(map_europe, xlim = c(-20, 59), ylim = c(35, 71), asp = 1)
points(geo_corpus$longtitude, geo_corpus$latitude, col = "red", cex = .6) 

library(ggmap)
map_world <- get_map(zoom = 3)
mapPoints <- ggmap(map_world) +
  geom_point(aes(x = longtitude, y = latitude, colour = query), data = geo_corpus)
mapPoints

world <- map_data("world")
worldmap <- ggplot() +
  geom_path(data=world, aes(x=long, y=lat, group=group),size=0.5,colour="grey") +
  geom_point(data=geo_corpus,aes(x = longtitude, y = latitude, colour = query)) +
  coord_equal() + 
  theme_nothing(legend=TRUE) +
  theme(
    legend.position=c(0.1,0.5)
  )
worldmap
  
map_eu <- get_map(zoom = 4, location= "Europe")
mapPoints <- ggmap(map_eu) +
  geom_point(aes(x = longtitude, y = latitude, colour = query), data = geo_corpus)
mapPoints
```


## Inferential Statistics

# Sentiment Analysis

For sentiment analysis, we used the R packages tm.lexicon.GeneralInquirer [@R-tm.lexicon.GeneralInquirer] and tm.plugin.sentiment [@R-tm.plugin.sentiment]. For those tweets for which we have at the time of writing this document already had downlaoded location information (Googlemaps API restricts requests to a daily maximum and the Geomap API restricts requests to a monthly maximum) we calculated the sentiment from the text corpora based on simple word counts. The package already includes a sentiment dictionary. The sentiment analysis returned a number in between -3 and 3 for all tweets. Low scores indicate negative sentiment, high scores indicate positive sentiment. 

## References

