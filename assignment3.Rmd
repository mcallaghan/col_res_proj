---
title: 'Twitter and the European Hyperagora: What can the Twittersphere Tell us about
  Political Deliberation and Opinions in Europe?'
author: "M. Callaghan, V. Nieberg"
date: "November 14, 2015"
output: html_document
bibliography:
- bibliography.bib
- RpackageCitations3.bib
---

```{r include=FALSE}
pkgs <- c('twitteR', 'tm.lexicon.GeneralInquirer', 'tm.plugin.sentiment','dplyr')
repmis::LoadandCite(pkgs, file = 'RpackageCitations3.bib')
```

# Data Gathering 

Due to the large amount of data we process, we ran the data gathering and cleaning in the background on a server using the prefix setsid.


## Tweets
We used a modified version of GetOldTweets [@GetOldTweets], a Java program that scrapes data from twitter search. The file getting_tweets/input.txt contains a list of search terms related to the Greek crisis in three periods, each comprising some weeks before and after the negotiation and signing of the memoranda. The search terms were collected using an adapted form of snowball sampling [@snowball], searching an initial list and recursively adding related terms found in the results. By running 
```
sudo setsid ./compile_run.sh ../getting_tweets/input.txt
```
from the GetOldTweets folder, we ran through each search term and each period, searched twitter, and saved the results as a txt file in the data folder. After an initial assessment of the results, we refined our search terms and ran GetOldTweets again with /getting_tweets/input2.txt. A third file (getting_tweets/input3.txt) aims to return a time-inpedependent list of tweets in order to control for the growth of Twitter over time.

We end up with a long list of files in the data/GOToutput folder, which in the data cleaning process will be merged into one corpus file.

## Users
We found the unique users in our corpus of tweets and used the TwitteR package [@R-twitteR] to gather richer data about each user. TwitteR uses the twitter API and gives the opportunity to collect all information twitter has abou the user. Where a users's last tweet was geocoded, we took the latitude and longtitude. We end up with the file data/user_info.csv

Many users do not geotag their tweets, instead stating their location, and we used APIs from MapQuest and Google to geocode user-reported location, giving us the file places.csv.


# Data Cleaning  
The txt files containing the tweets for each query and period are merged into a corpus file. This corpus file was merged with the user_info file, which in turn was merged with the places file. We end up with a large file containing tweets for our queries in each period with elaborete user information.

Some of the queries we defined returned irrelevant data, due to their ambiguity. We identified these by selecting random tweets from the search queries, reading the tweets, and checking for relevance to the topic. For example, the query "bailout", although certainly relevant for our topic, was insufficiently precise and returned a lot of data about the banking bailouts, especially in the 2010 period.

The following list summarizes the queries which we excluded.

* athens
* bailout
* 2-pac
* 3-pac



```{r cache=TRUE}
user_info <- read.csv('data/user_info.csv')
corpus <- read.csv('data/corpus.csv') %>%
  filter(query!="2-pac" & query != "3-pac", query!="bailout" & query != "athens")
geo_user_info <- user_info[!is.na(user_info$latitude), ]
geo_corpus <- merge(corpus, geo_user_info)

```


# Data Analysis

## Descriptive Statistics


### Summary Tables

The followign table give a first overview over the collected date. The first table shows absolute numbers and relative distributions of specific query returns. The second table describes distribution of specific tweets over time (normalized???). The third table describes geographical distribution of tweets for those tweets that we have at the time of writing this been able to obtain information on location. ^[API from Google restricts requests to a daily maximum and the API from MapQuest restricts requests to a monthly maximum]. 


```{r cache=TRUE}

sum_q_table <- corpus %>%
  group_by(query) %>%
  summarise(
    n = length(query),
    percent = n/length(corpus$tweet_id)*100
  )

knitr::kable(sum_q_table, digits = 2)

```


2. table queries by periods, absolute numbers, ratio of period occurances

```{r cache=TRUE}

sum_q_per_table <- corpus %>%
  group_by(query, period) %>%
  summarise(
    n = length(query),
    percent = n/length(corpus[corpus$query == query,"tweet_id"])*100
  )

knitr::kable(sum_q_per_table, digits = 2)

```

3. table for where we have the location
how many in which countries, percentage distribution over countries

```{r cache=TRUE}

sum_q_loc_table <- senti_geo_corpus %>%
  group_by(approx_country, period) %>%
  summarise(
    n = length(query),
    percent = n/length(corpus$tweet_id)
  )

knitr::kable(sum_q_loc_table, digits = 2)

```

### Timeless against other queries

### Summary Charts

```{r cache=TRUE}
library(ggplot2)
library(dplyr)
library(stringr)
library(wordcloud)
library(tm)
corpus <- read.csv('data/corpus.csv')
eurocrisis <- filter(corpus, query == 'eurocrisis')

corpus <- filter(corpus,query!="2-pac"&query!="3-pac")

eurocrisis$day <- as.Date(substr(as.character(eurocrisis$date),1,10))

eurocrisis_daily <- eurocrisis %>%
  group_by(day) %>%
  summarise(
    n = length(day)
  )

corpus$day <- as.Date(substr(as.character(corpus$date),1,10))

corpus_index <- corpus %>%
  group_by(query) %>%
  summarise(total = length(query))

corpus_index_large <- filter(corpus_index,total>10000)
corpus_index_small <- filter(corpus_index,total<10000)

corpus_large <- filter(corpus,query %in% corpus_index_large$query)

corpus_small <- filter(corpus,query %in% corpus_index_small$query)

timegraph <- function(corpus,p) {
  corpus_daily <- corpus %>%
    filter(period==p) %>%
  group_by(day,query,period) %>%
  summarise(
    n = length(day)
  )
  

ggplot(filter(corpus_daily), aes(day,n,colour=query)) + geom_line() +
  scale_x_date(format("%b-%Y")) +
  ggtitle(p)

}

timegraph(corpus_small,"2010-04-15-2010-05-15.txt")
timegraph(corpus_small,"2012-02-07-2012-03-31.txt")
timegraph(corpus_small,"2015-07-01-2015-08-25.txt")

timegraph(corpus_large,"2010-04-15-2010-05-15.txt")
timegraph(corpus_large,"2012-02-07-2012-03-31.txt")
timegraph(corpus_large,"2015-07-01-2015-08-25.txt")



```

### Word cloud
```{r cache = TRUE}
minicorpus <- corpus[sample(nrow(corpus), 200), ]

text_corpus <- Corpus(VectorSource(minicorpus$text))

#clean up
text_corpus <- tm_map(text_corpus,
                              content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')),
                              mc.cores=1
)
text_corpus <- tm_map(text_corpus, content_transformer(tolower), mc.cores=1)
text_corpus <- tm_map(text_corpus, removePunctuation, mc.cores=1)
text_corpus <- tm_map(text_corpus, function(x)removeWords(x,stopwords()), mc.cores=1)
wordcloud(text_corpus)

```
The word cloud gives us an overall picture of the words used by twitter users tweeting about the European debt-crisis. To a certain degree, this word cloud is redundant in that it returns our query turns. However, it also indicates the prevalence of specific terms, and further, which other terms are mentioned regularly in those tweets. 


### Map
```{r}
library(rworldmap)
map_world <- getMap(resolution = "high")
plot(map_world, asp = 1)
points(geo_corpus$longtitude, geo_corpus$latitude, col = "red", cex = .6) 

map_europe <- getMap(resolution = "high")
plot(map_europe, xlim = c(-20, 59), ylim = c(35, 71), asp = 1)
points(geo_corpus$longtitude, geo_corpus$latitude, col = "red", cex = .6) 

library(ggmap)
map_world <- get_map(zoom = 3)
mapPoints <- ggmap(map_world) +
  geom_point(aes(x = longtitude, y = latitude, colour = query), data = geo_corpus)
mapPoints

world <- map_data("world")
worldmap <- ggplot() +
  geom_path(data=world, aes(x=long, y=lat, group=group),size=0.5,colour="grey") +
  geom_point(data=geo_corpus,aes(x = longtitude, y = latitude, colour = query)) +
  coord_equal() + 
  theme_nothing(legend=TRUE) +
  theme(
    legend.position=c(0.1,0.5)
  )
worldmap

```

This map shows the distribution of individual queries over different locations worldwide. Colour indicates type of query. Concerning the 


```{r}  
map_eu <- get_map(zoom = 4, location= "Europe")
mapPoints <- ggmap(map_eu) +
  geom_point(aes(x = longtitude, y = latitude, colour = query), data = geo_corpus)
mapPoints
```

The following maps show the distribution of individual queries over different locations Europe-wide. Colour indicated type of query. We produced two maps differentiating between frequently used and less frequently used terms. 



The following map shows the three periods studied for one specific query. 

```{r include=FALSE, eval=FALSE}  
map_eu_2010 <- get_map(zoom = 4, location= "Europe")
mapPoints <- ggmap(map_eu_2010) +
  geom_point(aes(x = longtitude, y = latitude, colour = query), data = DATAFRAME1)
mapPoints

map_eu_2012 <- get_map(zoom = 4, location= "Europe")
mapPoints <- ggmap(map_eu_2012) +
  geom_point(aes(x = longtitude, y = latitude, colour = query), data = DATAFRAME2)
mapPoints

map_eu_2015 <- get_map(zoom = 4, location= "Europe")
mapPoints <- ggmap(map_eu_2015) +
  geom_point(aes(x = longtitude, y = latitude, colour = query), data = DATAFRAME3)
mapPoints
```

# Inferential Statistics

## Sentiment Analysis

For sentiment analysis, we used the R packages tm.lexicon.GeneralInquirer [@R-tm.lexicon.GeneralInquirer] and tm.plugin.sentiment [@R-tm.plugin.sentiment]. For those tweets for which we have at the time of writing this document already had downlaoded location information we calculated the sentiment from the text corpora based on simple word counts. The package already includes a sentiment dictionary. The sentiment analysis returned a number in between -3 and 3 for all tweets. Low scores indicate negative sentiment, high scores indicate positive sentiment.

We looked at individual tweets and compared the results to our perception of sentiment of the tweet.

We created a map which colour-coded avarage sentiment for European countries. 



# References

